#!/usr/bin/env python
# coding: utf-8

# # Data Analysis with pandas 2
# 
# If you do machine learning or other forms of AI-based data analysis, Big Data analysis, then you will quickly experience that about 80% of the time you will spend is not on the analysis itself, but on the data preparation, cleaning and reorganization, reconfiguration to get "all your ducks in a row".
# 
# During the first week of the course, we had a quick glance at pandas to perform basic data analysis from a dataset I provided you with.

# In[1]:


import pandas as pd


#   In pandas, there are two main types of data, the series and the dataframe. You can simply generate a series as demonstrated underneath. If you are not familiar with Python, remember that it starts counting at "0" and not 1.

# In[2]:


dat = pd.Series([4, 3, 5, 9, -9])
dat


# In pandas, the data is structured with an index. You can define the column as an index or you can just use the one that is generated by default. You can extract information about the index using the following command:

# In[3]:


dat.index


# You can define the index or use a column of data so that after definding it, if you call it, you will have the index defined with names.
# 

# In[4]:


dat.index = ['robert','mario','Yusuke','Chen', 'Chris']
dat.index


# Because it is the index, you can then call rows of data based on the index variables:

# In[8]:


dat['mario']


# Now, we could play with dummy small datasets forever, but pandas is particularly useful when it comes to large datasets because it is very swift. We are going to open the csv files of the number of deaths for disasters related to natural hazards (there is not such a thing as a natural disaster) generated from the EMDAT database and we will make sure that it is converted into a Pandas Dataframe by using the DataFrame function.

# In[110]:


data = pd.read_csv("number-of-deaths-from-natural-disasters.csv")


# In[111]:


data.head


# In[112]:


data = pd.DataFrame(data)


# In[113]:


data


# Now, one thing that you want to do is to create an index with the different types of hazards... 
# We are then going to make an index column from the Entity column, and then call one of the element of the index.

# In[114]:


data.index = data.Entity
data.loc['Wildfire']


# We can also separate the Wildfire from the rest of the dataset by creating a "Wildfire dataset"

# In[32]:


WildfireData = data.loc['Wildfire']


# In[34]:


WildfireData.head()


# Now that we have only wildfire, we can drop the column that we don't need.

# In[47]:


WildfireData = WildfireData.loc[:,['Year',"Total deaths (EMDAT (2020))"]]
WildfireData.head()


# The name of the column is way toooooo long so that we may want to find something easier like (Deaths) and to do that we just have to rename the column, and this is one way to do it:

# In[54]:


WildfireData = WildfireData.rename(columns={"Total deaths (EMDAT (2020))":"Deaths"})
WildfireData.head()


# In[61]:


WildfireData.mean()


# In[62]:


WildfireData.sum()


# In[63]:


WildfireData.min()


# In[64]:


WildfireData.max()


# You can also integrate those statistics as a new column in your dataset. For example you may want to get the cummulated data, which could then be added to the original pandas dataframe. In such case, you calculate the values you want to create or it can be another dataset, and then you add it to the WildfireData by specifying the name of the column you want to use to save it. Et voila. This is the kind of table handling pandas is really fast at doing.

# In[67]:


CumDeaths = WildfireData['Deaths'].cumsum()


# In[70]:


WildfireData['CumDeaths'] = CumDeaths


# In[71]:


WildfireData.head()


# Now that we have our datasets, we can have a look at them and also investigate them by using graphic means, and here are two simple graphics created with the matplotlib library pyplot. This is close to what we have done the first time, but I am introducing you here with a bit more options and the possibility to make several sub-graphics on one main graphic.

# In[98]:


import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [20, 5]
fig = plt.figure()
ax1 = fig.add_subplot(1,2,1)
_ = ax1.hist(WildfireData['Deaths'], bins = 40, color = 'k', alpha = 0.5)
ax1.set_title("Histograms of casualties per year due to wildfires")
ax2 = fig.add_subplot(1,2,2)
_ = ax2.plot(WildfireData['Year'],WildfireData['CumDeaths'],'k--',label = 'cummulated deaths')
_ = ax2.plot(WildfireData['Year'],WildfireData['Deaths'], color = 'r', label = 'deaths per year')
ax2.legend(loc='best')
ax2.set_title("Wildfire casualties between 1918 and 2010")
plt.savefig('WildfireCasualties.jpg')


# 
# Now that we have worked with Wildfire, which I took because I could see it in my table construction, what about the other values, do I need to stream through the all index to know what is in there? Well, not really. Again, there is a very useful tool for that (it exists in numpy as well), it is the unique tool, so that for the index, we can ask pandas to extract all the unique values or names in the column.
# 

# In[105]:


data.index.unique()


# In the same way that you can extract index or work on columns as we have done above, you can also extract information by year for instance, as follows:

# In[122]:


data = data.rename(columns={"Total deaths (EMDAT (2020))":"Deaths"})
data[data.Year==1995]


# Let say that we want to do is to look at the correlation between extreme temperature and wildfire from the death toll perspective, and let say that we are only interested in years where deaths have occurred in both the cases of wildfire and extreme temperature, we would need to merge the two datasets by dates and for only dates that both of them have occurred. Here is a way to do so:
# 

# In[134]:


ExtTemp = data.loc['Extreme temperature']
data2 = pd.merge(WildfireData, ExtTemp, on=["Year"])
data2 = data2.rename(columns={"Deaths_x":"WildFire_death","Deaths_y":"ExtTemp_death"})


# In[135]:


data2.head()


# In[140]:


plt.scatter(data2.WildFire_death,data2.ExtTemp_death)


# ... and we can then provide conditions to our plot as well, by let say plotting data only when the extreme temperature have taken less than 11k people.

# In[139]:


plt.scatter(data2.WildFire_death[data2.ExtTemp_death<11000],data2.ExtTemp_death[data2.ExtTemp_death<11000])


# In the example above, there is no clear pattern that occurs (at least from just visualizing the data). Now, your turn, try to have a play with the different elements of the dataset and see if you can create visualization, different statistical summary of the data, which I will invite you to present in a minute.
